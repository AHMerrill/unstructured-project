{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AHMerrill/unstructured-project/blob/resolve_topic_assignment_2/anti_echo_chamber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test cell"
      ],
      "metadata": {
        "id": "EcSTm50mDW8Y"
      },
      "id": "EcSTm50mDW8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "# Anti Echo Chamber ‚Äî Full Analysis and Retrieval Pipeline\n",
        "# ================================================================\n",
        "\n",
        "This notebook performs a complete, production-grade anti-echo workflow:\n",
        "\n",
        "1. Secure OpenAI login  \n",
        "2. Rebuild ChromaDB from Hugging Face  \n",
        "3. Upload and parse PDF / TXT / HTML  \n",
        "4. Summarize with OpenAI (`gpt-4o-mini`)  \n",
        "5. Create topic + stance embeddings  \n",
        "6. Compare against Chroma to surface ideologically contrasting articles  \n",
        "\n",
        "Repositories  \n",
        "- GitHub: https://github.com/AHMerrill/anti-echo-chamber  \n",
        "- Hugging Face dataset: https://huggingface.co/datasets/zanimal/anti-echo-artifacts\n"
      ],
      "metadata": {
        "id": "BBCWCGQG50vz"
      },
      "id": "BBCWCGQG50vz"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 1 ‚Äî Secure OpenAI API Key Setup\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "print(\"OpenAI API key loaded into environment (hidden)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BfTTprh8Srm",
        "outputId": "f11104dd-a88a-43c8-dd58-27611b119635"
      },
      "id": "9BfTTprh8Srm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded into environment (hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "# Stage 2 ‚Äî Environment Setup and Repository Configuration\n",
        "# ================================================================\n",
        "\n",
        "This stage:\n",
        "- Clones the GitHub repo\n",
        "- Installs dependencies\n",
        "- Loads YAML + JSON configs\n",
        "- Prints the active models and Chroma settings\n"
      ],
      "metadata": {
        "id": "ktl-Qobe8WUM"
      },
      "id": "ktl-Qobe8WUM"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 2 ‚Äî Environment Setup and Repository Configuration (config-driven)\n",
        "# ================================================================\n",
        "\n",
        "import os, json, yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Paths ---\n",
        "PROJECT_ROOT = Path(\"/content/unstructured-project\").resolve()\n",
        "\n",
        "# --- Clone correct repo if missing ---\n",
        "if not PROJECT_ROOT.exists():\n",
        "    print(\"Cloning repo from GitHub...\")\n",
        "    os.system(\"git clone https://github.com/AHMerrill/unstructured-project.git /content/unstructured-project\")\n",
        "else:\n",
        "    print(\"Repository exists. Pulling latest changes...\")\n",
        "    os.system(f\"cd {PROJECT_ROOT} && git pull\")\n",
        "\n",
        "# --- Load config.yaml from correct path ---\n",
        "CONFIG_PATH = PROJECT_ROOT / \"config/config.yaml\"\n",
        "if not CONFIG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Config file not found at: {CONFIG_PATH}\")\n",
        "\n",
        "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "# --- Install dependencies ---\n",
        "!pip install -q pdfplumber beautifulsoup4 chromadb sentence-transformers pyyaml huggingface_hub openai rapidfuzz\n",
        "\n",
        "summary = {\n",
        "    \"repo\": \"AHMerrill/unstructured-project\",\n",
        "    \"hf_dataset_id\": CONFIG.get(\"hf_dataset_id\"),\n",
        "    \"topic_model\": CONFIG.get(\"embeddings\", {}).get(\"topic_model\"),\n",
        "    \"stance_model\": CONFIG.get(\"embeddings\", {}).get(\"stance_model\"),\n",
        "    \"chroma_collections\": CONFIG.get(\"chroma_collections\"),\n",
        "}\n",
        "print(json.dumps(summary, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khFeOzme8dH1",
        "outputId": "1f62ccfa-97d4-4989-c76d-5d47b7c9a412"
      },
      "id": "khFeOzme8dH1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository exists. Pulling latest changes...\n",
            "{\n",
            "  \"repo\": \"AHMerrill/unstructured-project\",\n",
            "  \"hf_dataset_id\": \"zanimal/unstructured-project\",\n",
            "  \"topic_model\": \"intfloat/e5-base-v2\",\n",
            "  \"stance_model\": \"all-mpnet-base-v2\",\n",
            "  \"chroma_collections\": {\n",
            "    \"topic\": \"unstructured_topic\",\n",
            "    \"stance\": \"unstructured_stance\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "# Stage 3 ‚Äî Full Chroma Rebuild from Hugging Face Dataset\n",
        "# ================================================================\n",
        "\n",
        "This stage reconstructs the local **ChromaDB** from your Hugging Face dataset  \n",
        "[`zanimal/anti-echo-artifacts`](https://huggingface.co/datasets/zanimal/anti-echo-artifacts).\n",
        "\n",
        "It preserves the full multi-topic and multi-stance structure from your scraper:\n",
        "- Each article can yield **multiple topic vectors** (`::topic::0`, `::topic::1`, ‚Ä¶)\n",
        "- Each article can yield **multiple stance vectors** (`::stance::summary`, `::stance::0`, ‚Ä¶)\n",
        "\n",
        "Duplicates are filtered **only by exact row_id**, not by base article ID.  \n",
        "This ensures we retain all topical clusters while preventing re-ingestion of the same batch.\n"
      ],
      "metadata": {
        "id": "_n2MIXWK8i5a"
      },
      "id": "_n2MIXWK8i5a"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 3 ‚Äî Full Chroma Rebuild from Hugging Face Dataset (config-driven)\n",
        "# ================================================================\n",
        "\n",
        "import os, json, numpy as np, traceback\n",
        "from pathlib import Path\n",
        "from huggingface_hub import list_repo_files, hf_hub_download\n",
        "import chromadb\n",
        "from collections import defaultdict\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/unstructured-project\").resolve()\n",
        "CHROMA_PATH  = PROJECT_ROOT / \"chroma_db\"\n",
        "CHROMA_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "HF_REPO = CONFIG[\"hf_dataset_id\"]\n",
        "topic_name  = CONFIG[\"chroma_collections\"][\"topic\"]\n",
        "stance_name = CONFIG[\"chroma_collections\"][\"stance\"]\n",
        "\n",
        "client = chromadb.PersistentClient(path=str(CHROMA_PATH))\n",
        "\n",
        "# Drop and recreate clean collections\n",
        "for name in [topic_name, stance_name]:\n",
        "    try:\n",
        "        client.delete_collection(name)\n",
        "    except Exception:\n",
        "        pass\n",
        "topic_coll  = client.create_collection(topic_name,  metadata={\"hnsw:space\": \"cosine\"})\n",
        "stance_coll = client.create_collection(stance_name, metadata={\"hnsw:space\": \"cosine\"})\n",
        "print(f\"Initialized collections '{topic_name}' and '{stance_name}' at {CHROMA_PATH}\")\n",
        "\n",
        "# --- Helpers (unchanged) ---\n",
        "def load_npz_safely(path):\n",
        "    arr = np.load(path, allow_pickle=False)\n",
        "    if isinstance(arr, np.lib.npyio.NpzFile):\n",
        "        for key in arr.files:\n",
        "            if arr[key].ndim == 2:\n",
        "                return arr[key]\n",
        "        raise ValueError(f\"No 2D arrays found in {path}\")\n",
        "    return arr\n",
        "\n",
        "def load_jsonl(fp):\n",
        "    records = []\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line: continue\n",
        "            try: records.append(json.loads(line))\n",
        "            except: continue\n",
        "    return records\n",
        "\n",
        "# --- Discover batches ---\n",
        "files = list_repo_files(HF_REPO, repo_type=\"dataset\")\n",
        "batches = sorted({\"/\".join(f.split(\"/\")[:2]) for f in files if f.startswith(\"batches/\")})\n",
        "print(f\"Detected {len(batches)} batches in {HF_REPO}\")\n",
        "\n",
        "# --- Rebuild loop (same logic as before) ---\n",
        "seen_topic_ids, seen_stance_ids = set(), set()\n",
        "article_topic_counts, article_stance_counts = defaultdict(int), defaultdict(int)\n",
        "topic_total = stance_total = 0\n",
        "\n",
        "for batch in batches:\n",
        "    try:\n",
        "        print(f\"\\n--- Processing {batch} ---\")\n",
        "        topic_npz  = hf_hub_download(HF_REPO, f\"{batch}/embeddings_topic.npz\",  repo_type=\"dataset\")\n",
        "        stance_npz = hf_hub_download(HF_REPO, f\"{batch}/embeddings_stance.npz\", repo_type=\"dataset\")\n",
        "        meta_topic = hf_hub_download(HF_REPO, f\"{batch}/metadata_topic.jsonl\",  repo_type=\"dataset\")\n",
        "        meta_stance= hf_hub_download(HF_REPO, f\"{batch}/metadata_stance.jsonl\", repo_type=\"dataset\")\n",
        "\n",
        "        t_embs, s_embs = load_npz_safely(topic_npz), load_npz_safely(stance_npz)\n",
        "        t_meta, s_meta = load_jsonl(meta_topic),  load_jsonl(meta_stance)\n",
        "\n",
        "        # topic upsert\n",
        "        t_records=[]\n",
        "        for e,m in zip(t_embs,t_meta):\n",
        "            rid=m.get(\"row_id\") or f\"{m.get('id','unknown')}::topic::0\"\n",
        "            if rid in seen_topic_ids: continue\n",
        "            seen_topic_ids.add(rid); t_records.append((rid,e,m))\n",
        "            article_topic_counts[rid.split('::')[0]]+=1\n",
        "        if t_records:\n",
        "            topic_coll.upsert(\n",
        "                ids=[r[0] for r in t_records],\n",
        "                embeddings=[r[1].tolist() for r in t_records],\n",
        "                metadatas=[r[2] for r in t_records])\n",
        "        topic_total+=len(t_records)\n",
        "\n",
        "        # stance upsert\n",
        "        s_records=[]\n",
        "        for e,m in zip(s_embs,s_meta):\n",
        "            rid=m.get(\"row_id\") or f\"{m.get('id','unknown')}::stance::0\"\n",
        "            if rid in seen_stance_ids: continue\n",
        "            seen_stance_ids.add(rid); s_records.append((rid,e,m))\n",
        "            article_stance_counts[rid.split('::')[0]]+=1\n",
        "        if s_records:\n",
        "            stance_coll.upsert(\n",
        "                ids=[r[0] for r in s_records],\n",
        "                embeddings=[r[1].tolist() for r in s_records],\n",
        "                metadatas=[r[2] for r in s_records])\n",
        "        stance_total+=len(s_records)\n",
        "        print(f\"‚úì Added {len(t_records)} topic, {len(s_records)} stance vectors\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed {batch}: {type(e).__name__}: {e}\")\n",
        "        traceback.print_exc(limit=1)\n",
        "\n",
        "print(\"\\n=== Rebuild Summary ===\")\n",
        "print(f\"Topic vectors added: {topic_total}\")\n",
        "print(f\"Stance vectors added: {stance_total}\")\n",
        "print(f\"Stored at {CHROMA_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2NotSan8mGR",
        "outputId": "9dfa8eb2-4d10-4dd3-bfea-e9c6c2fd1052"
      },
      "id": "R2NotSan8mGR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized collections 'unstructured_topic' and 'unstructured_stance' at /content/unstructured-project/chroma_db\n",
            "Detected 2 batches in zanimal/unstructured-project\n",
            "\n",
            "--- Processing batches/batch_20251025T173713Z_5dd9b9dd ---\n",
            "‚úì Added 64 topic, 13 stance vectors\n",
            "\n",
            "--- Processing batches/batch_20251025T181241Z_d8804259 ---\n",
            "‚úì Added 2104 topic, 419 stance vectors\n",
            "\n",
            "=== Rebuild Summary ===\n",
            "Topic vectors added: 2168\n",
            "Stance vectors added: 432\n",
            "Stored at /content/unstructured-project/chroma_db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "# Stage 4 ‚Äî User Upload + Source Bias Detection\n",
        "# ================================================================\n",
        "\n",
        "This stage handles ingestion of a user-supplied article (TXT / PDF / HTML).  \n",
        "It performs four key steps:\n",
        "\n",
        "1. **Upload** the file and extract plain text.  \n",
        "2. **Infer or confirm** the publication source.  \n",
        "3. **Match** against existing entries in `source_bias.json` (fuzzy).  \n",
        "4. **If new**, infer ideological metadata (bias family + score) using `gpt-4o-mini`.\n",
        "\n",
        "All extracted and inferred data will be cached for later topic + stance analysis.\n"
      ],
      "metadata": {
        "id": "2NxjvGuBBt1Y"
      },
      "id": "2NxjvGuBBt1Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 4 ‚Äî User Upload + Source Bias Detection\n",
        "# ================================================================\n",
        "\n",
        "import os, re, json, pdfplumber, requests\n",
        "from bs4 import BeautifulSoup\n",
        "from rapidfuzz import process, fuzz\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "# --- Environment setup ---\n",
        "PROJECT_ROOT = Path(\"/content/unstructured-project\").resolve()\n",
        "CONFIG_DIR = PROJECT_ROOT / \"config\"\n",
        "\n",
        "# Load your bias map\n",
        "SOURCE_BIAS_PATH = CONFIG_DIR / \"source_bias.json\"\n",
        "SOURCE_BIAS = json.load(open(SOURCE_BIAS_PATH, encoding=\"utf-8\"))\n",
        "\n",
        "# Ensure OpenAI key\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"].strip():\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1. File Upload + Text Extraction\n",
        "# --------------------------------------------------------------------\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "file_ext = Path(filename).suffix.lower()\n",
        "\n",
        "def extract_text(path):\n",
        "    if path.endswith(\".txt\"):\n",
        "        return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    if path.endswith(\".pdf\"):\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    if path.endswith(\".html\") or path.endswith(\".htm\"):\n",
        "        soup = BeautifulSoup(Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\"), \"html.parser\")\n",
        "        for s in soup([\"script\",\"style\"]): s.decompose()\n",
        "        return soup.get_text(separator=\" \")\n",
        "    raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "article_text = extract_text(filename).strip()\n",
        "print(f\"Extracted {len(article_text)} characters from {filename}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2. Attempt Source Inference\n",
        "# --------------------------------------------------------------------\n",
        "# Heuristic: find common domains in text, or ask GPT if ambiguous\n",
        "def infer_source_name(text):\n",
        "    # Quick domain sniff\n",
        "    m = re.search(r\"https?://([^/\\s]+)\", text)\n",
        "    if m:\n",
        "        domain = m.group(1).lower()\n",
        "        domain = domain.replace(\"www.\", \"\")\n",
        "        return domain.split(\".\")[0]\n",
        "    # GPT fallback\n",
        "    prompt = (\n",
        "        \"You are a media analyst. Based on the article text below, \"\n",
        "        \"infer the most likely publication or outlet name. \"\n",
        "        \"Return only the outlet name, e.g., 'The Guardian', 'Fox News', etc.\\n\\n\"\n",
        "        f\"Article excerpt:\\n{text[:2000]}\"\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=20,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    guess = resp.choices[0].message.content.strip()\n",
        "    return re.sub(r\"[^A-Za-z0-9\\s\\-]\", \"\", guess)\n",
        "\n",
        "inferred_source = infer_source_name(article_text)\n",
        "print(f\"üïµÔ∏è Inferred possible source: {inferred_source}\")\n",
        "\n",
        "# Ask user to confirm or override\n",
        "user_resp = input(f\"Is this article from '{inferred_source}'? [y/n]: \").strip().lower()\n",
        "if user_resp != \"y\":\n",
        "    user_source = input(\"Enter the source name (may contain typos): \").strip()\n",
        "    confirmed_source = user_source\n",
        "else:\n",
        "    confirmed_source = inferred_source\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3. Fuzzy Match Against Known Sources\n",
        "# --------------------------------------------------------------------\n",
        "known_sources = list(SOURCE_BIAS.keys())\n",
        "match, score, _ = process.extractOne(confirmed_source, known_sources, scorer=fuzz.ratio)\n",
        "print(f\"Closest match: {match} (score {score})\")\n",
        "\n",
        "if score >= 85:\n",
        "    bias_info = SOURCE_BIAS[match]\n",
        "    print(f\"Matched existing bias entry for {match}\")\n",
        "else:\n",
        "    # ----------------------------------------------------------------\n",
        "    # 4. GPT Bias Inference Fallback\n",
        "    # ----------------------------------------------------------------\n",
        "    prompt = f\"\"\"\n",
        "You are a media bias researcher.\n",
        "Given the outlet name \"{confirmed_source}\", infer its general political bias family\n",
        "(e.g., 'center left', 'center right', 'libertarian right', 'progressive left', 'neutral').\n",
        "Return JSON with:\n",
        "- bias_family\n",
        "- bias_score (float, -1.0 = far left, +1.0 = far right)\n",
        "- short_rationale (brief explanation)\n",
        "\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=CONFIG[\"stance_processing\"][\"llm\"][\"model\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=128,\n",
        "        temperature=0.4\n",
        "    )\n",
        "    try:\n",
        "        bias_info = json.loads(resp.choices[0].message.content)\n",
        "    except Exception:\n",
        "        bias_info = {\"bias_family\": \"unknown\", \"bias_score\": 0.0, \"short_rationale\": resp.choices[0].message.content.strip()}\n",
        "    print(f\"New outlet inferred:\\n{json.dumps(bias_info, indent=2)}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 5. Cache Inferred Metadata\n",
        "# --------------------------------------------------------------------\n",
        "ARTICLE_META = {\n",
        "    \"filename\": filename,\n",
        "    \"source_input\": confirmed_source,\n",
        "    \"matched_source\": match if score >= 85 else None,\n",
        "    \"bias_family\": bias_info.get(\"bias_family\", \"unknown\"),\n",
        "    \"bias_score\": bias_info.get(\"bias_score\", 0.0),\n",
        "    \"rationale\": bias_info.get(\"short_rationale\", \"\"),\n",
        "    \"chars\": len(article_text)\n",
        "}\n",
        "\n",
        "# Save to temporary workspace\n",
        "TEMP_DIR = PROJECT_ROOT / \"tmp\"\n",
        "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "meta_path = TEMP_DIR / f\"{Path(filename).stem}_meta.json\"\n",
        "text_path = TEMP_DIR / f\"{Path(filename).stem}.txt\"\n",
        "Path(text_path).write_text(article_text, encoding=\"utf-8\")\n",
        "Path(meta_path).write_text(json.dumps(ARTICLE_META, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n--- Source Bias Summary ---\")\n",
        "print(json.dumps(ARTICLE_META, indent=2))\n",
        "print(f\"Cached article + metadata under {TEMP_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "Fqq7T08tBvoO",
        "outputId": "6cfbbf88-47aa-402c-e1e5-3c82a66c0636"
      },
      "id": "Fqq7T08tBvoO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b482b989-24c0-47da-9606-bb3987027c62\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b482b989-24c0-47da-9606-bb3987027c62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fox_test.pdf to fox_test (1).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19461 characters from fox_test (1).pdf\n",
            "üïµÔ∏è Inferred possible source: foxnews\n",
            "Is this article from 'foxnews'? [y/n]: y\n",
            "Closest match: foxnews (score 100.0)\n",
            "Matched existing bias entry for foxnews\n",
            "\n",
            "--- Source Bias Summary ---\n",
            "{\n",
            "  \"filename\": \"fox_test (1).pdf\",\n",
            "  \"source_input\": \"foxnews\",\n",
            "  \"matched_source\": \"foxnews\",\n",
            "  \"bias_family\": \"conservative right\",\n",
            "  \"bias_score\": 0.8,\n",
            "  \"rationale\": \"\",\n",
            "  \"chars\": 19461\n",
            "}\n",
            "Cached article + metadata under /content/unstructured-project/tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "# Stage 5a ‚Äî Topic Embedding with Composite Vector (Base + GPT Summary)\n",
        "# ================================================================\n",
        "\n",
        "This step creates a **two-layer topic representation** for precise matching:\n",
        "\n",
        "### What it does:\n",
        "1. **Loads** the extracted text and metadata from Stage 4\n",
        "2. **Generates base topic embeddings** via hierarchical clustering (same as scraper)\n",
        "3. **Assigns canonical topics** from the predefined taxonomy (e.g., \"Politics / Global / Geopolitics & Conflict\")\n",
        "4. **NEW: Generates GPT summary** - A one-sentence description of the article's specific subject (e.g., \"Trump imposes sanctions on Russian oil companies amid Ukraine conflict\")\n",
        "5. **Creates composite embedding** by concatenating:\n",
        "   - Base topic vector (768 dimensions) - captures broad semantic topic\n",
        "   - GPT summary vector (768 dimensions) - captures specific subject details\n",
        "   - Final output: 1536-dimensional composite vector\n",
        "\n",
        "### Output files:\n",
        "- `{article}_topic_composite.npy` - Composite 1536-dim vector (used for retrieval)\n",
        "- `{article}_topic.npy` - Base topic vectors (for backward compatibility)\n",
        "- `{article}_topics_flat.json` - List of canonical topic labels\n",
        "- `{article}_topic_summary.json` - GPT summary + metadata\n"
      ],
      "metadata": {
        "id": "MC7rpNxNEfas"
      },
      "id": "MC7rpNxNEfas"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 5a ‚Äî Topic Embedding + Canonical Topic Assignment + GPT Summary (Composite)\n",
        "# ================================================================\n",
        "\n",
        "import json, numpy as np, nltk, torch, os\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/unstructured-project\").resolve()\n",
        "CONFIG_DIR = PROJECT_ROOT / \"config\"\n",
        "TMP = PROJECT_ROOT / \"tmp\"\n",
        "EPHEMERAL = TMP / \"ephemeral_embeddings\"\n",
        "EPHEMERAL.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- OpenAI setup ---\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"].strip():\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# --- Load text + meta ---\n",
        "latest_meta = sorted(TMP.glob(\"*_meta.json\"))[-1]\n",
        "meta = json.load(open(latest_meta))\n",
        "text_path = TMP / f\"{Path(latest_meta).stem.replace('_meta','')}.txt\"\n",
        "article_text = text_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Embedding topics for: {meta['filename']} ({len(article_text)} chars)\")\n",
        "\n",
        "# --- Config parameters ---\n",
        "topic_model_name = CONFIG[\"embeddings\"][\"topic_model\"]\n",
        "chunk_tokens = CONFIG[\"embeddings\"][\"chunk_tokens\"]\n",
        "normalize = CONFIG[\"embeddings\"][\"normalize\"]\n",
        "threshold = CONFIG[\"topics\"][\"similarity_threshold\"]\n",
        "max_topics_per_vec = CONFIG[\"topics\"][\"max_topics_per_article\"]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using topic model: {topic_model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(topic_model_name, use_fast=True)\n",
        "embedder = SentenceTransformer(topic_model_name, device=device)\n",
        "\n",
        "# --- Load canonical topic anchors + labels ---\n",
        "anchors_path = CONFIG_DIR / \"topic_anchors.npz\"\n",
        "topics_path = CONFIG_DIR / \"topics.json\"\n",
        "anchors_npz = np.load(anchors_path, allow_pickle=True)\n",
        "topic_anchors = {k: anchors_npz[k] for k in anchors_npz.files}\n",
        "topic_labels = list(topic_anchors.keys())\n",
        "print(f\"Loaded {len(topic_anchors)} topic anchors\")\n",
        "\n",
        "# --- NLTK setup ---\n",
        "for pkg in [\"punkt\", \"punkt_tab\"]:\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "    except LookupError:\n",
        "        nltk.download(pkg)\n",
        "\n",
        "def sent_split(text):\n",
        "    return [s.strip() for s in nltk.sent_tokenize(text) if s.strip()]\n",
        "\n",
        "def encode(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    vecs = embedder.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=normalize,\n",
        "        show_progress_bar=False,\n",
        "    )\n",
        "    return np.array(vecs)\n",
        "\n",
        "def topic_vecs(text):\n",
        "    sents = sent_split(text)\n",
        "    if not sents:\n",
        "        return []\n",
        "    if len(sents) < 2:\n",
        "        return [encode(\" \".join(sents)).mean(axis=0)]\n",
        "    emb = encode(sents)\n",
        "    k = min(max(1, len(sents)//8), 8)\n",
        "    labels = AgglomerativeClustering(n_clusters=k).fit_predict(emb)\n",
        "    segs = [\" \".join([s for s, l in zip(sents, labels) if l == lab]) for lab in sorted(set(labels))]\n",
        "    out = []\n",
        "    for seg in segs:\n",
        "        out.append(encode(seg).mean(axis=0))\n",
        "    return out\n",
        "\n",
        "def match_topics(vec, anchors_dict, max_topics=5, threshold=0.4):\n",
        "    scores = {\n",
        "        label: float(cosine_similarity([vec], [anchor])[0][0])\n",
        "        for label, anchor in anchors_dict.items()\n",
        "    }\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected = []\n",
        "    for i, (label, sim) in enumerate(ranked[:max_topics]):\n",
        "        if i == 0 or sim >= threshold:\n",
        "            selected.append({\"topic_label\": label, \"similarity\": sim})\n",
        "    if not selected:\n",
        "        selected = [{\"topic_label\": \"General / Miscellaneous\", \"similarity\": 0.0}]\n",
        "    return selected\n",
        "\n",
        "# --- Generate base topic embeddings + topic matches ---\n",
        "topic_vec_list = topic_vecs(article_text)\n",
        "topic_vecs_array = np.vstack(topic_vec_list)\n",
        "print(f\"Generated {len(topic_vecs_array)} base topic embeddings with shape {topic_vecs_array.shape}\")\n",
        "\n",
        "# --- Match to anchors ---\n",
        "all_labels = []\n",
        "topic_matches = []\n",
        "for i, vec in enumerate(topic_vecs_array):\n",
        "    matches = match_topics(vec, topic_anchors, max_topics=max_topics_per_vec, threshold=threshold)\n",
        "    topic_matches.append(matches)\n",
        "    top_labels = [m[\"topic_label\"] for m in matches]\n",
        "    all_labels.extend(top_labels)\n",
        "    print(f\"\\n[Topic vector {i}] matches:\")\n",
        "    for m in matches:\n",
        "        print(f\"  - {m['topic_label']:<40} (similarity {m['similarity']:.3f})\")\n",
        "\n",
        "# --- Deduplicate + limit to top 8 overall topics ---\n",
        "flat_topics = list(dict.fromkeys(all_labels))[:8]\n",
        "print(\"\\n--- Canonical Topics Assigned ---\")\n",
        "for t in flat_topics:\n",
        "    print(f\" - {t}\")\n",
        "\n",
        "# ================================================================\n",
        "# NEW: Generate GPT summary for fine-grained topic matching\n",
        "# ================================================================\n",
        "print(\"\\n--- Generating GPT Topic Summary ---\")\n",
        "\n",
        "summary_prompt = f\"\"\"\n",
        "Summarize this article's specific subject in ONE sentence (max 20 words).\n",
        "Be concrete and specific about what event, person, location, or issue is discussed.\n",
        "Focus on the WHO/WHAT/WHERE, not opinion or analysis.\n",
        "\n",
        "Article title: {meta.get('filename', 'Unknown')}\n",
        "Text excerpt: {article_text[:2500]}\n",
        "\n",
        "Example good summaries:\n",
        "- \"Discusses Israel's military operations in Gaza and humanitarian impact\"\n",
        "- \"Analyzes Federal Reserve's interest rate decision and inflation outlook\"\n",
        "- \"Covers Elon Musk's acquisition of Twitter and content moderation changes\"\n",
        "- \"Reports on Supreme Court ruling on affirmative action in college admissions\"\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
        "        max_tokens=40,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    topic_summary = response.choices[0].message.content.strip()\n",
        "    # Clean up any quotes or extra formatting\n",
        "    topic_summary = topic_summary.strip('\"').strip(\"'\").strip()\n",
        "    print(f\"Topic summary: {topic_summary}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: GPT summary failed ({e}), using title as fallback\")\n",
        "    topic_summary = meta.get('filename', 'Unknown article')\n",
        "\n",
        "# Embed the summary\n",
        "summary_vec = encode(topic_summary)  # shape (1, 768)\n",
        "\n",
        "# Flatten to 1D if needed\n",
        "if summary_vec.ndim == 2:\n",
        "    summary_vec = summary_vec.flatten()  # Now shape (768,)\n",
        "\n",
        "print(f\"Summary embedding shape: {summary_vec.shape}\")\n",
        "\n",
        "# ================================================================\n",
        "# Create composite topic vector: [base_topic_embedding | summary_embedding]\n",
        "# ================================================================\n",
        "# Use the first (primary) base topic vector\n",
        "primary_topic_vec = topic_vecs_array[0]  # shape (768,)\n",
        "\n",
        "# Ensure both are 1D\n",
        "if primary_topic_vec.ndim == 2:\n",
        "    primary_topic_vec = primary_topic_vec.flatten()\n",
        "\n",
        "# Concatenate: base topic (768) + summary (768) = composite (1536)\n",
        "composite_topic_vec = np.concatenate([primary_topic_vec, summary_vec])\n",
        "print(f\"\\nComposite topic vector shape: {composite_topic_vec.shape}\")\n",
        "print(f\"  - Base topic portion: indices 0-767\")\n",
        "print(f\"  - Summary portion: indices 768-1535\")\n",
        "\n",
        "# --- Save all outputs ---\n",
        "base = Path(meta[\"filename\"]).stem\n",
        "\n",
        "# Save composite vector (for retrieval)\n",
        "composite_path = EPHEMERAL / f\"{base}_topic_composite.npy\"\n",
        "np.save(composite_path, composite_topic_vec)\n",
        "\n",
        "# Save base topic vectors (for backward compatibility/debugging)\n",
        "topic_path = EPHEMERAL / f\"{base}_topic.npy\"\n",
        "np.save(topic_path, topic_vecs_array)\n",
        "\n",
        "# Save metadata\n",
        "match_path = EPHEMERAL / f\"{base}_topic_matches.json\"\n",
        "flat_path = EPHEMERAL / f\"{base}_topics_flat.json\"\n",
        "summary_path = EPHEMERAL / f\"{base}_topic_summary.json\"\n",
        "\n",
        "json.dump(topic_matches, open(match_path, \"w\"), indent=2)\n",
        "json.dump(flat_topics, open(flat_path, \"w\"), indent=2)\n",
        "json.dump({\n",
        "    \"summary\": topic_summary,\n",
        "    \"embedding_version\": \"v2_composite\",\n",
        "    \"composite_shape\": list(composite_topic_vec.shape),\n",
        "    \"base_shape\": list(primary_topic_vec.shape),\n",
        "    \"summary_shape\": list(summary_vec.shape)\n",
        "}, open(summary_path, \"w\"), indent=2)\n",
        "\n",
        "print(f\"\\n--- Saved Artifacts ---\")\n",
        "print(f\"Composite topic vector ‚Üí {composite_path}\")\n",
        "print(f\"Base topic vectors ‚Üí {topic_path}\")\n",
        "print(f\"Topic matches ‚Üí {match_path}\")\n",
        "print(f\"Canonical topics ‚Üí {flat_path}\")\n",
        "print(f\"Topic summary metadata ‚Üí {summary_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qwaCAhrEjsE",
        "outputId": "89bf9da5-3f0d-4da3-d46a-da70d2eb836a"
      },
      "id": "-qwaCAhrEjsE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding topics for: fox_test.pdf (19461 chars)\n",
            "Using topic model: intfloat/e5-base-v2\n",
            "Loaded 22 topic anchors\n",
            "Generated 8 base topic embeddings with shape (8, 768)\n",
            "\n",
            "[Topic vector 0] matches:\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.818)\n",
            "  - Technology / Social Media & Platforms    (similarity 0.812)\n",
            "  - Economy / Trade / Globalization          (similarity 0.809)\n",
            "  - Society / Media / Communication          (similarity 0.807)\n",
            "  - Environment / Climate / Energy Policy    (similarity 0.805)\n",
            "\n",
            "[Topic vector 1] matches:\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.804)\n",
            "  - Economy / Trade / Globalization          (similarity 0.795)\n",
            "  - Politics / US / Federal / Executive Policy (similarity 0.783)\n",
            "  - Environment / Climate / Energy Policy    (similarity 0.776)\n",
            "  - Technology / Social Media & Platforms    (similarity 0.773)\n",
            "\n",
            "[Topic vector 2] matches:\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.809)\n",
            "  - Politics / US / Federal / Executive Policy (similarity 0.778)\n",
            "  - Economy / Trade / Globalization          (similarity 0.774)\n",
            "  - Politics / US / Federal / Elections and Campaigns (similarity 0.767)\n",
            "  - Ethics / Governance & Accountability / Integrity (similarity 0.763)\n",
            "\n",
            "[Topic vector 3] matches:\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.816)\n",
            "  - Politics / US / Federal / Executive Policy (similarity 0.795)\n",
            "  - Economy / Trade / Globalization          (similarity 0.791)\n",
            "  - Economy / Finance / Markets              (similarity 0.791)\n",
            "  - Politics / US / Federal / Elections and Campaigns (similarity 0.777)\n",
            "\n",
            "[Topic vector 4] matches:\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.815)\n",
            "  - Economy / Trade / Globalization          (similarity 0.793)\n",
            "  - Politics / US / Federal / Executive Policy (similarity 0.793)\n",
            "  - Technology / Social Media & Platforms    (similarity 0.781)\n",
            "  - Politics / US / Federal / Elections and Campaigns (similarity 0.777)\n",
            "\n",
            "[Topic vector 5] matches:\n",
            "  - Politics / US / Federal / Executive Policy (similarity 0.808)\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.805)\n",
            "  - Politics / US / Federal / Elections and Campaigns (similarity 0.797)\n",
            "  - Ethics / Governance & Accountability / Integrity (similarity 0.794)\n",
            "  - Environment / Disaster & Resilience      (similarity 0.789)\n",
            "\n",
            "[Topic vector 6] matches:\n",
            "  - Sports / Global Athletics / Competitions (similarity 0.812)\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.811)\n",
            "  - Technology / Social Media & Platforms    (similarity 0.798)\n",
            "  - Technology / Innovation / AI             (similarity 0.796)\n",
            "  - Law / Legal Systems / Justice            (similarity 0.794)\n",
            "\n",
            "[Topic vector 7] matches:\n",
            "  - Politics / Global / Geopolitics & Conflict (similarity 0.806)\n",
            "  - Economy / Trade / Globalization          (similarity 0.798)\n",
            "  - Politics / US / Federal / Executive Policy (similarity 0.791)\n",
            "  - Ethics / Governance & Accountability / Integrity (similarity 0.786)\n",
            "  - Technology / Social Media & Platforms    (similarity 0.782)\n",
            "\n",
            "--- Canonical Topics Assigned ---\n",
            " - Politics / Global / Geopolitics & Conflict\n",
            " - Technology / Social Media & Platforms\n",
            " - Economy / Trade / Globalization\n",
            " - Society / Media / Communication\n",
            " - Environment / Climate / Energy Policy\n",
            " - Politics / US / Federal / Executive Policy\n",
            " - Politics / US / Federal / Elections and Campaigns\n",
            " - Ethics / Governance & Accountability / Integrity\n",
            "\n",
            "--- Generating GPT Topic Summary ---\n",
            "Topic summary: Trump imposes sanctions on Russian oil companies Rosneft and Lukoil and cancels summit with Putin amid Ukraine conflict.\n",
            "Summary embedding shape: (768,)\n",
            "\n",
            "Composite topic vector shape: (1536,)\n",
            "  - Base topic portion: indices 0-767\n",
            "  - Summary portion: indices 768-1535\n",
            "\n",
            "--- Saved Artifacts ---\n",
            "Composite topic vector ‚Üí /content/unstructured-project/tmp/ephemeral_embeddings/fox_test_topic_composite.npy\n",
            "Base topic vectors ‚Üí /content/unstructured-project/tmp/ephemeral_embeddings/fox_test_topic.npy\n",
            "Topic matches ‚Üí /content/unstructured-project/tmp/ephemeral_embeddings/fox_test_topic_matches.json\n",
            "Canonical topics ‚Üí /content/unstructured-project/tmp/ephemeral_embeddings/fox_test_topics_flat.json\n",
            "Topic summary metadata ‚Üí /content/unstructured-project/tmp/ephemeral_embeddings/fox_test_topic_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 5b ‚Äî Stance Classification + Hybrid Embedding (Ephemeral, Scraper-Accurate)\n",
        "# ================================================================\n",
        "\n",
        "import os, json, re, torch, numpy as np\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/unstructured-project\").resolve()\n",
        "CONFIG_DIR   = PROJECT_ROOT / \"config\"\n",
        "TMP          = PROJECT_ROOT / \"tmp\"\n",
        "EPHEMERAL    = TMP / \"ephemeral_embeddings\"\n",
        "EPHEMERAL.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Load configs and guides ---\n",
        "with open(CONFIG_DIR / \"political_leanings.json\", encoding=\"utf-8\") as f:\n",
        "    leanings_map = json.load(f)\n",
        "with open(CONFIG_DIR / \"implied_stances.json\", encoding=\"utf-8\") as f:\n",
        "    stances_map = json.load(f)\n",
        "with open(CONFIG_DIR / \"source_bias.json\", encoding=\"utf-8\") as f:\n",
        "    source_bias = json.load(f)\n",
        "\n",
        "# --- Load metadata and article text from Stage 4 ---\n",
        "latest_meta = sorted(TMP.glob(\"*_meta.json\"))[-1]\n",
        "meta        = json.load(open(latest_meta))\n",
        "text_path   = TMP / f\"{Path(latest_meta).stem.replace('_meta','')}.txt\"\n",
        "article_txt = text_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Generating stance embedding for: {meta['filename']} ({len(article_txt)} chars)\")\n",
        "\n",
        "# --- Retrieve outlet bias already inferred in Stage 4 ---\n",
        "bias_family = meta.get(\"bias_family\", \"unknown\")\n",
        "bias_score  = float(meta.get(\"bias_score\", 0.0))\n",
        "\n",
        "# --- OpenAI client ---\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"].strip():\n",
        "    from getpass import getpass\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# --- GPT classification identical to scraper ---\n",
        "prompt = f\"\"\"\n",
        "You are a political analyst.\n",
        "Based on the article below, classify its overall political leaning (tone) and implied stance.\n",
        "\n",
        "Leaning options: {', '.join(leanings_map.keys())}\n",
        "Stance examples: {', '.join([s for cat in stances_map.values() for s in cat['families'].keys()])}\n",
        "\n",
        "Return strict JSON with fields:\n",
        "- political_leaning (string)\n",
        "- implied_stance (string)\n",
        "- summary (one-sentence summary of the article's main argument)\n",
        "\n",
        "Article title: {meta.get('filename')}\n",
        "Excerpt: {article_txt[:2000]}\n",
        "\"\"\"\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=CONFIG[\"stance_processing\"][\"llm\"][\"model\"],\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=256,\n",
        "    temperature=0.4\n",
        ")\n",
        "raw = resp.choices[0].message.content.strip()\n",
        "\n",
        "try:\n",
        "    stance_info = json.loads(raw)\n",
        "except Exception:\n",
        "    # fallback regex extraction\n",
        "    leaning = re.search(r\"leaning[:\\-]?\\s*(.+)\", raw, re.I)\n",
        "    stance  = re.search(r\"stance[:\\-]?\\s*(.+)\",  raw, re.I)\n",
        "    summary = re.search(r\"summary[:\\-]?\\s*(.+)\", raw, re.I)\n",
        "    stance_info = {\n",
        "        \"political_leaning\": (leaning.group(1).strip() if leaning else \"unknown\"),\n",
        "        \"implied_stance\":    (stance.group(1).strip()  if stance  else \"unknown\"),\n",
        "        \"summary\":           (summary.group(1).strip() if summary else raw[:200])\n",
        "    }\n",
        "\n",
        "print(\"\\n--- GPT Classification ---\")\n",
        "print(json.dumps(stance_info, indent=2))\n",
        "\n",
        "# --- Compute tone score + match with outlet bias ---\n",
        "def bias_to_score(label):\n",
        "    l = (label or \"\").lower().strip()\n",
        "    if \"progressive\" in l or (\"left\" in l and \"center\" not in l): return -0.8\n",
        "    if \"center left\" in l:  return -0.4\n",
        "    if l == \"center\":       return 0.0\n",
        "    if \"center right\" in l: return 0.4\n",
        "    if \"conservative\" in l or \"right\" in l: return 0.8\n",
        "    if \"libertarian\" in l:  return 0.6\n",
        "    return 0.0\n",
        "\n",
        "tone_score = bias_to_score(stance_info.get(\"political_leaning\"))\n",
        "author_match = abs(bias_score - tone_score) <= 0.3\n",
        "\n",
        "# --- Build enriched stance metadata ---\n",
        "stance_meta = {\n",
        "    \"political_leaning\": stance_info.get(\"political_leaning\", \"unknown\"),\n",
        "    \"implied_stance\":    stance_info.get(\"implied_stance\", \"unknown\"),\n",
        "    \"summary\":           stance_info.get(\"summary\", \"\"),\n",
        "    \"bias_family\":       bias_family,\n",
        "    \"bias_score\":        bias_score,\n",
        "    \"tone_score\":        tone_score,\n",
        "    \"author_tone_match\": author_match\n",
        "}\n",
        "\n",
        "print(\"\\n--- Source / Tone Alignment ---\")\n",
        "print(json.dumps(stance_meta, indent=2))\n",
        "\n",
        "# --- Hybrid text for embedding ---\n",
        "hybrid_text = \"\\n\".join([\n",
        "    stance_meta[\"political_leaning\"],\n",
        "    stance_meta[\"implied_stance\"],\n",
        "    stance_meta[\"summary\"]\n",
        "]).strip()\n",
        "\n",
        "# --- Generate embedding ---\n",
        "stance_model_name = CONFIG[\"embeddings\"][\"stance_model\"]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedder = SentenceTransformer(stance_model_name, device=device)\n",
        "stance_vec = embedder.encode(hybrid_text, normalize_embeddings=True)\n",
        "stance_vec = stance_vec.reshape(1, -1)\n",
        "print(f\"\\nUsing stance model: {stance_model_name}\")\n",
        "print(f\"Generated stance vector with shape {stance_vec.shape}\")\n",
        "\n",
        "# --- Save ephemeral outputs for Stage 6 ---\n",
        "base = Path(meta[\"filename\"]).stem\n",
        "np.save(EPHEMERAL / f\"{base}_stance.npy\", stance_vec)\n",
        "Path(EPHEMERAL / f\"{base}_stance_summary.txt\").write_text(hybrid_text, encoding=\"utf-8\")\n",
        "Path(EPHEMERAL / f\"{base}_stance_info.json\").write_text(json.dumps(stance_meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\nSaved ephemeral stance artifacts under {EPHEMERAL}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c15hgzKYLH0M",
        "outputId": "44cc9db3-b3c4-4e0d-ecf8-9bf2b88ec872"
      },
      "id": "c15hgzKYLH0M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating stance embedding for: fox_test.pdf (19461 chars)\n",
            "\n",
            "--- GPT Classification ---\n",
            "{\n",
            "  \"political_leaning\": \"\\\": \\\"center right\\\",\",\n",
            "  \"implied_stance\": \"\\\": \\\"nationalist realist\\\",\",\n",
            "  \"summary\": \"\\\": \\\"The article discusses President Trump's imposition of sanctions on Russian oil companies as a strategic move to pressure the Kremlin while avoiding deeper U.S. involvement in the Ukraine conflict.\\\"\"\n",
            "}\n",
            "\n",
            "--- Source / Tone Alignment ---\n",
            "{\n",
            "  \"political_leaning\": \"\\\": \\\"center right\\\",\",\n",
            "  \"implied_stance\": \"\\\": \\\"nationalist realist\\\",\",\n",
            "  \"summary\": \"\\\": \\\"The article discusses President Trump's imposition of sanctions on Russian oil companies as a strategic move to pressure the Kremlin while avoiding deeper U.S. involvement in the Ukraine conflict.\\\"\",\n",
            "  \"bias_family\": \"conservative right\",\n",
            "  \"bias_score\": 0.8,\n",
            "  \"tone_score\": 0.4,\n",
            "  \"author_tone_match\": false\n",
            "}\n",
            "\n",
            "Using stance model: all-mpnet-base-v2\n",
            "Generated stance vector with shape (1, 768)\n",
            "\n",
            "Saved ephemeral stance artifacts under /content/unstructured-project/tmp/ephemeral_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 6 ‚Äî Retrieval and Anti-Echo Analysis\n",
        "\n",
        "This stage retrieves articles related to the uploaded article and groups them by stance similarity and topic relevance.\n",
        "\n",
        "---\n",
        "\n",
        "### Process Summary\n",
        "\n",
        "* **Load uploaded article features**:\n",
        "\n",
        "  * Topic embedding (composite of topic + summary)\n",
        "  * Bias and tone scores (from stance metadata)\n",
        "* **Retrieve corpus candidates** from Chroma using topic similarity.\n",
        "* **Compute comparison metrics** for each candidate:\n",
        "\n",
        "  * **summary_similarity** ‚Äì cosine similarity between summary/topic embeddings\n",
        "  * **canonical_topic_overlap** ‚Äì Jaccard overlap of canonical topic tags\n",
        "  * **stance_similarity** ‚Äì cosine similarity between stance embeddings\n",
        "  * **bias_diff** ‚Äì absolute difference in bias scores\n",
        "  * **tone_diff** ‚Äì absolute difference in tone scores\n",
        "\n",
        "---\n",
        "\n",
        "### Ranking Logic\n",
        "\n",
        "Articles are grouped and ranked in tiers:\n",
        "\n",
        "| Category                        | Criteria                   | Sort Key                                      |\n",
        "| ------------------------------- | -------------------------- | --------------------------------------------- |\n",
        "| **Same topic, similar stance**  | High summary similarity    | summary_similarity ‚Üì                          |\n",
        "| **Same topic, opposite stance** | High topic relevance       | stance_similarity ‚Üë then summary_similarity ‚Üì |\n",
        "| **Different source bias**       | Bias significantly differs | bias_diff ‚Üì then summary_similarity ‚Üì         |\n",
        "| **Mixed/neutral**               | Sorted by hybrid signal    | anti_echo_score ‚Üì                             |\n",
        "\n",
        "---\n",
        "\n",
        "### Scoring Formula\n",
        "\n",
        "```\n",
        "anti_echo_score =\n",
        "    (w_topic  * summary_similarity)\n",
        "  - (w_stance * stance_similarity)\n",
        "  + (w_bias   * bias_diff)\n",
        "  + (w_tone   * tone_diff)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Output\n",
        "\n",
        "* **Printed sections**:\n",
        "\n",
        "  * Same topic, similar stance\n",
        "  * Same topic, opposite stance\n",
        "  * Different source bias\n",
        "  * Full ranked list\n",
        "* **CSV export** of metrics:\n",
        "\n",
        "  * summary_similarity\n",
        "  * canonical_topic_overlap\n",
        "  * stance_similarity\n",
        "  * bias_diff\n",
        "  * tone_diff\n",
        "  * anti_echo_score"
      ],
      "metadata": {
        "id": "lXd0b52ZGCbr"
      },
      "id": "lXd0b52ZGCbr"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Stage 6 ‚Äî Retrieval and Anti-Echo Analysis (Composite Topic + Stance)\n",
        "# ================================================================\n",
        "\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from getpass import getpass\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# TUNABLE PARAMETERS\n",
        "# ---------------------------------------------------------------\n",
        "# Composite score weights\n",
        "w_T_canonical = 0.5   # canonical topic overlap weight\n",
        "w_T_summary = 2.0     # summary similarity weight (NEW - weighted higher for precision)\n",
        "w_S = 1.0             # stance similarity penalty\n",
        "w_B = 0.8             # bias difference penalty\n",
        "w_Tone = 0.3          # tone difference penalty\n",
        "\n",
        "# Retrieval thresholds and ranking controls\n",
        "CANONICAL_TOPIC_THRESHOLD = 0.3   # minimum canonical overlap (lowered since summary helps)\n",
        "SUMMARY_SIMILARITY_THRESHOLD = 0.8  # minimum summary similarity\n",
        "TOP_N_RESULTS = 10              # how many top-ranked results to show in each section\n",
        "PRINT_TOP_N = 3                 # how many to print per section in console output\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Configuration and paths\n",
        "# ---------------------------------------------------------------\n",
        "PROJECT_ROOT = Path(\"/content/unstructured-project\").resolve()\n",
        "CHROMA_PATH = PROJECT_ROOT / \"chroma_db\"\n",
        "TMP = PROJECT_ROOT / \"tmp\"\n",
        "EPHEMERAL = TMP / \"ephemeral_embeddings\"\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# OpenAI setup\n",
        "# ---------------------------------------------------------------\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"].strip():\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Load metadata and ephemeral embeddings\n",
        "# ---------------------------------------------------------------\n",
        "latest_meta = sorted(TMP.glob(\"*_meta.json\"))[-1]\n",
        "meta = json.load(open(latest_meta))\n",
        "article_id = Path(meta[\"filename\"]).stem\n",
        "bias_score_article = float(meta[\"bias_score\"])\n",
        "tone_score_article = float(meta.get(\"tone_score\", 0.0))\n",
        "print(f\"Running anti-echo retrieval for {meta['filename']} (bias={bias_score_article}, tone={tone_score_article})\")\n",
        "\n",
        "# Load composite topic vector\n",
        "composite_topic_vec = np.load(EPHEMERAL / f\"{article_id}_topic_composite.npy\")\n",
        "uploaded_base_topic = composite_topic_vec[:768]    # base topic embedding\n",
        "uploaded_summary = composite_topic_vec[768:]        # summary embedding\n",
        "\n",
        "# Load other data\n",
        "stance_vec = np.load(EPHEMERAL / f\"{article_id}_stance.npy\")\n",
        "topics_flat = json.load(open(EPHEMERAL / f\"{article_id}_topics_flat.json\"))\n",
        "topic_summary_meta = json.load(open(EPHEMERAL / f\"{article_id}_topic_summary.json\"))\n",
        "topic_summary = topic_summary_meta.get(\"summary\", \"\")\n",
        "\n",
        "print(f\"\\nUploaded article topic summary: {topic_summary}\")\n",
        "print(f\"Composite vector shape: {composite_topic_vec.shape}\")\n",
        "print(f\"  Base topic: {uploaded_base_topic.shape}\")\n",
        "print(f\"  Summary: {uploaded_summary.shape}\")\n",
        "\n",
        "client_chroma = chromadb.PersistentClient(path=str(CHROMA_PATH))\n",
        "topic_coll = client_chroma.get_collection(CONFIG[\"chroma_collections\"][\"topic\"])\n",
        "stance_coll = client_chroma.get_collection(CONFIG[\"chroma_collections\"][\"stance\"])\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Helpers\n",
        "# ---------------------------------------------------------------\n",
        "def parse_topics(obj):\n",
        "    if obj is None:\n",
        "        return []\n",
        "    if isinstance(obj, list):\n",
        "        return [t.strip() for t in obj if t.strip()]\n",
        "    if isinstance(obj, str):\n",
        "        parts = [t.strip() for t in obj.split(\";\") if t.strip()]\n",
        "        if len(parts) == 1 and parts[0].startswith(\"[\"):\n",
        "            try:\n",
        "                parsed = json.loads(parts[0])\n",
        "                if isinstance(parsed, list):\n",
        "                    return [t.strip() for t in parsed if isinstance(t, str)]\n",
        "            except Exception:\n",
        "                pass\n",
        "        return parts\n",
        "    return []\n",
        "\n",
        "def topic_overlap_score(a_topics, b_topics):\n",
        "    \"\"\"Canonical topic overlap (Jaccard similarity)\"\"\"\n",
        "    a = set([t.strip().lower() for t in parse_topics(a_topics)])\n",
        "    b = set([t.strip().lower() for t in parse_topics(b_topics)])\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "    return len(a & b) / len(a | b)\n",
        "\n",
        "def interpret_bias(score: float) -> str:\n",
        "    if score <= -0.6: return \"Progressive / Left\"\n",
        "    if -0.6 < score <= -0.2: return \"Center-Left\"\n",
        "    if -0.2 < score < 0.2: return \"Center / Neutral\"\n",
        "    if 0.2 <= score < 0.6: return \"Center-Right\"\n",
        "    if score >= 0.6: return \"Conservative / Right\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "def short_url(u, max_len=70):\n",
        "    if not u:\n",
        "        return \"\"\n",
        "    return (u[:max_len] + \"‚Ä¶\") if len(u) > max_len else u\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Retrieve all topic & stance docs\n",
        "# ---------------------------------------------------------------\n",
        "topic_docs = topic_coll.get(include=[\"embeddings\", \"metadatas\"])\n",
        "stance_docs = stance_coll.get(include=[\"embeddings\", \"metadatas\"])\n",
        "\n",
        "# Count unique articles (not topic vectors)\n",
        "unique_articles_in_db = len(set(\n",
        "    md.get(\"id\", \"\").split(\"::\")[0]\n",
        "    for md in topic_docs[\"metadatas\"]\n",
        "))\n",
        "\n",
        "print(f\"\\nRetrieved from Chroma:\")\n",
        "print(f\"  - {len(topic_docs['ids'])} topic vectors\")\n",
        "print(f\"  - {len(stance_docs['ids'])} stance vectors\")\n",
        "print(f\"  - {unique_articles_in_db} unique articles\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Compare uploaded article to stored corpus (NEW: Handle mixed formats)\n",
        "# ---------------------------------------------------------------\n",
        "print(f\"\\nScanning corpus for matches...\")\n",
        "print(f\"Note: Database contains mix of old (768-dim) and new (1536-dim) embeddings\")\n",
        "\n",
        "# Load sentence transformer for on-the-fly encoding of old summaries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "topic_model_name = CONFIG[\"embeddings\"][\"topic_model\"]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "topic_embedder = SentenceTransformer(topic_model_name, device=device)\n",
        "\n",
        "def encode_text(text):\n",
        "    \"\"\"Encode text and return flattened vector\"\"\"\n",
        "    vec = topic_embedder.encode(text, normalize_embeddings=True, show_progress_bar=False)\n",
        "    if vec.ndim == 2:\n",
        "        vec = vec.flatten()\n",
        "    return vec\n",
        "\n",
        "# First pass: collect all matches (including duplicates from multiple topic vectors)\n",
        "all_matches = []\n",
        "\n",
        "for emb, md in zip(topic_docs[\"embeddings\"], topic_docs[\"metadatas\"]):\n",
        "    # ========================================\n",
        "    # STAGE 1: Canonical topic overlap filter\n",
        "    # ========================================\n",
        "    canonical_overlap = topic_overlap_score(topics_flat, md.get(\"topics_flat\", []))\n",
        "\n",
        "    if canonical_overlap < CANONICAL_TOPIC_THRESHOLD:\n",
        "        continue  # Skip articles with no canonical topic overlap\n",
        "\n",
        "    # ========================================\n",
        "    # STAGE 2: Summary similarity matching (Handle old/new formats)\n",
        "    # ========================================\n",
        "    emb_array = np.array(emb)\n",
        "\n",
        "    if len(emb_array) == 1536:\n",
        "        # NEW FORMAT: Extract summary embedding from composite vector\n",
        "        candidate_summary_vec = emb_array[768:]\n",
        "        summary_similarity = cosine_similarity(\n",
        "            uploaded_summary.reshape(1, -1),\n",
        "            candidate_summary_vec.reshape(1, -1)\n",
        "        )[0][0]\n",
        "    else:\n",
        "        # OLD FORMAT: Check if metadata has openai_topic_summary\n",
        "        old_summary = md.get(\"openai_topic_summary\", \"\")\n",
        "\n",
        "        if old_summary:\n",
        "            # Encode the old summary text and compare\n",
        "            candidate_summary_vec = encode_text(old_summary)\n",
        "            summary_similarity = cosine_similarity(\n",
        "                uploaded_summary.reshape(1, -1),\n",
        "                candidate_summary_vec.reshape(1, -1)\n",
        "            )[0][0]\n",
        "        else:\n",
        "            # Fallback: use full topic vector (less precise)\n",
        "            summary_similarity = cosine_similarity(\n",
        "                uploaded_base_topic.reshape(1, -1),\n",
        "                emb_array.reshape(1, -1)\n",
        "            )[0][0]\n",
        "\n",
        "    # Filter by summary similarity threshold\n",
        "    if summary_similarity < SUMMARY_SIMILARITY_THRESHOLD:\n",
        "        continue  # Skip articles that aren't similar enough in specific topic\n",
        "\n",
        "    # Store match with article ID for deduplication\n",
        "    article_id = md.get(\"id\", \"\").split(\"::\")[0]  # Extract base article ID (before ::topic::N)\n",
        "\n",
        "    all_matches.append({\n",
        "        \"article_id\": article_id,\n",
        "        \"row_id\": md.get(\"id\"),  # Full ID with topic vector index\n",
        "        \"source\": md.get(\"source\", \"\"),\n",
        "        \"title\": md.get(\"title\", \"\"),\n",
        "        \"url\": md.get(\"url\", \"\"),\n",
        "        \"bias_family\": md.get(\"bias_family\", \"\"),\n",
        "        \"topics_flat\": md.get(\"topics_flat\", []),\n",
        "        \"canonical_overlap\": canonical_overlap,\n",
        "        \"summary_similarity\": summary_similarity,\n",
        "        \"candidate_summary\": old_summary if len(emb_array) != 1536 else \"(new format)\",\n",
        "        \"metadata\": md\n",
        "    })\n",
        "\n",
        "print(f\"Found {len(all_matches)} topic vector matches from {len(set(m['article_id'] for m in all_matches))} unique articles\")\n",
        "\n",
        "# ========================================\n",
        "# Deduplicate: Keep best topic vector per article\n",
        "# ========================================\n",
        "best_matches = {}\n",
        "for match in all_matches:\n",
        "    aid = match[\"article_id\"]\n",
        "\n",
        "    # Keep the match with highest summary similarity for each article\n",
        "    if aid not in best_matches or match[\"summary_similarity\"] > best_matches[aid][\"summary_similarity\"]:\n",
        "        best_matches[aid] = match\n",
        "\n",
        "print(f\"After deduplication: {len(best_matches)} unique articles\")\n",
        "\n",
        "# ========================================\n",
        "# Now compute stance, bias, and final scores for deduplicated articles\n",
        "# ========================================\n",
        "scores = []\n",
        "\n",
        "for aid, match in best_matches.items():\n",
        "    md = match[\"metadata\"]\n",
        "\n",
        "    # Get bias and tone info from stance collection\n",
        "    bias_db, tone_db = 0.0, 0.0\n",
        "    for s_md in stance_docs[\"metadatas\"]:\n",
        "        # Match on base article ID (stance IDs also have ::stance:: suffix)\n",
        "        s_aid = s_md.get(\"id\", \"\").split(\"::\")[0]\n",
        "        if s_aid == aid:\n",
        "            try:\n",
        "                bias_db = float(s_md.get(\"bias_score\", 0.0))\n",
        "            except Exception:\n",
        "                try:\n",
        "                    bias_db = float(json.loads(s_md.get(\"source_bias\", \"{}\")).get(\"bias_score\", 0.0))\n",
        "                except Exception:\n",
        "                    bias_db = 0.0\n",
        "            tone_db = float(s_md.get(\"tone_score\", 0.0))\n",
        "            break\n",
        "\n",
        "    bias_diff = abs(bias_score_article - bias_db)\n",
        "    tone_diff = abs(tone_score_article - tone_db)\n",
        "\n",
        "    # Get stance embedding\n",
        "    stance_match = None\n",
        "    for s_emb, s_md in zip(stance_docs[\"embeddings\"], stance_docs[\"metadatas\"]):\n",
        "        s_aid = s_md.get(\"id\", \"\").split(\"::\")[0]\n",
        "        if s_aid == aid:\n",
        "            stance_match = s_emb\n",
        "            break\n",
        "\n",
        "    stance_sim = 0.0\n",
        "    if stance_match is not None:\n",
        "        stance_sim = cosine_similarity(\n",
        "            stance_vec.reshape(1, -1),\n",
        "            np.array(stance_match).reshape(1, -1)\n",
        "        )[0][0]\n",
        "\n",
        "    # ========================================\n",
        "    # Composite anti-echo score (NEW: includes summary similarity)\n",
        "    # ========================================\n",
        "    anti_echo_score = (\n",
        "        (w_T_canonical * match[\"canonical_overlap\"]) +\n",
        "        (w_T_summary * match[\"summary_similarity\"]) -\n",
        "        (w_S * stance_sim) -\n",
        "        (w_B * bias_diff) -\n",
        "        (w_Tone * tone_diff)\n",
        "    )\n",
        "\n",
        "    scores.append({\n",
        "        \"article_id\": aid,\n",
        "        \"source\": match[\"source\"],\n",
        "        \"title\": match[\"title\"],\n",
        "        \"url\": match[\"url\"],\n",
        "        \"bias_family\": match[\"bias_family\"],\n",
        "        \"bias_score\": bias_db,\n",
        "        \"canonical_topic_overlap\": match[\"canonical_overlap\"],\n",
        "        \"summary_similarity\": match[\"summary_similarity\"],\n",
        "        \"stance_similarity\": stance_sim,\n",
        "        \"bias_diff\": bias_diff,\n",
        "        \"tone_diff\": tone_diff,\n",
        "        \"anti_echo_score\": anti_echo_score\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(scores)\n",
        "if df.empty:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WARNING: No related articles found\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"This could mean:\")\n",
        "    print(\"1. No articles in the database share similar topics\")\n",
        "    print(\"2. Thresholds are too strict:\")\n",
        "    print(f\"   - Canonical topic threshold: {CANONICAL_TOPIC_THRESHOLD}\")\n",
        "    print(f\"   - Summary similarity threshold: {SUMMARY_SIMILARITY_THRESHOLD}\")\n",
        "    print(\"\\nTry lowering thresholds or checking your canonical topics:\")\n",
        "    print(f\"Your topics: {topics_flat}\")\n",
        "    print(\"=\"*80)\n",
        "    raise ValueError(\"No related articles found\")\n",
        "\n",
        "print(f\"\\nFinal result: {len(df)} unique articles matched\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Ranking (UPDATED: More distinct sections + quality filtering)\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# 1. Same specific topic, maximize BIAS difference (cross-partisan perspectives)\n",
        "same_topic_diff_bias = df.sort_values(\n",
        "    [\"bias_diff\", \"summary_similarity\"],\n",
        "    ascending=[False, False]\n",
        ").head(TOP_N_RESULTS)\n",
        "\n",
        "# 2. Same specific topic, minimize STANCE similarity (opposing arguments)\n",
        "#same_topic_opposite_stance = df.sort_values(\n",
        "#    [\"stance_similarity\", \"summary_similarity\"],\n",
        "#    ascending=[True, False]  # Lower stance = more opposite viewpoints\n",
        "#).head(TOP_N_RESULTS)\n",
        "\n",
        "same_topic_opposite_stance = df[\n",
        "    (df[\"summary_similarity\"] >= SUMMARY_SIMILARITY_THRESHOLD) &\n",
        "    (df[\"canonical_topic_overlap\"] >= CANONICAL_TOPIC_THRESHOLD)\n",
        "].sort_values(\n",
        "    [\"stance_similarity\", \"summary_similarity\"],\n",
        "    ascending=[True, False]\n",
        ").head(TOP_N_RESULTS)\n",
        "\n",
        "# 3. Best anti-echo candidates (composite score only)\n",
        "anti_echo_best = df.sort_values(\"anti_echo_score\", ascending=False).head(TOP_N_RESULTS)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Readable, structured console summaries\n",
        "# ---------------------------------------------------------------\n",
        "def print_header(title):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(title.upper().center(80))\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "def format_article_row(row):\n",
        "    title = (row.get(\"title\") or \"Untitled\").strip()\n",
        "    source = row.get(\"source\", \"unknown\")\n",
        "    bias_label = interpret_bias(row[\"bias_score\"])\n",
        "    metrics = (\n",
        "        f\"Canonical overlap: {row['canonical_topic_overlap']:.2f}   \"\n",
        "        f\"Summary sim: {row['summary_similarity']:.2f}   \"\n",
        "        f\"Stance sim: {row['stance_similarity']:.2f}   \"\n",
        "        f\"Bias diff: {row['bias_diff']:.2f}\"\n",
        "    )\n",
        "    lines = [\n",
        "        f\"‚Ä¢ {title}\",\n",
        "        f\"  Source: {source}  ({bias_label})\",\n",
        "        f\"  {metrics}\",\n",
        "        f\"  Anti-echo score: {row['anti_echo_score']:.3f}\",\n",
        "    ]\n",
        "    url = short_url(row.get(\"url\", \"\"))\n",
        "    if url:\n",
        "        lines.append(f\"  Link: {url}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def show_results(df, title, n=PRINT_TOP_N):\n",
        "    print_header(title)\n",
        "    if df.empty:\n",
        "        print(\"  No matches found.\\n\")\n",
        "        return\n",
        "    for _, row in df.head(n).iterrows():\n",
        "        print(format_article_row(row))\n",
        "        print(\"-\" * 80)\n",
        "    print()\n",
        "\n",
        "def show_overview(df):\n",
        "    print_header(\"Ideological Spread Overview\")\n",
        "    left = df[df[\"bias_score\"] < -0.2][\"source\"].unique()\n",
        "    right = df[df[\"bias_score\"] > 0.2][\"source\"].unique()\n",
        "    print(f\"Left / progressive outlets : {', '.join(left) if len(left)>0 else 'none'}\")\n",
        "    print(f\"Right / conservative outlets: {', '.join(right) if len(right)>0 else 'none'}\\n\")\n",
        "\n",
        "    print(f\"Search parameters:\")\n",
        "    print(f\"  Your article summary: {topic_summary}\")\n",
        "    print(f\"  Canonical topics: {', '.join(topics_flat[:3])}{'...' if len(topics_flat) > 3 else ''}\")\n",
        "    print()\n",
        "\n",
        "    top = df.iloc[0]\n",
        "    print(f\"Top matching article: {top['source']} ({interpret_bias(top['bias_score'])})\")\n",
        "    print(f\"  Title: {(top['title'] or 'Untitled')[:60]}...\")\n",
        "    print(\n",
        "        f\"  Canonical overlap: {top['canonical_topic_overlap']:.2f}   \"\n",
        "        f\"Summary sim: {top['summary_similarity']:.2f}   \"\n",
        "        f\"Stance sim: {top['stance_similarity']:.2f}   \"\n",
        "        f\"Bias diff: {top['bias_diff']:.2f}\"\n",
        "    )\n",
        "    print(f\"  Anti-echo score: {top['anti_echo_score']:.3f}\")\n",
        "    print()\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Pretty console output\n",
        "# ---------------------------------------------------------------\n",
        "show_overview(anti_echo_best)\n",
        "show_results(same_topic_diff_bias, \"Same Specific Topic ‚Äî Different Source Bias\")\n",
        "show_results(same_topic_opposite_stance, \"Same Specific Topic ‚Äî Opposite Stance\")\n",
        "show_results(anti_echo_best, \"Top Anti-Echo Candidates (Best Overall Matches)\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Save results\n",
        "# ---------------------------------------------------------------\n",
        "out_path = TMP / f\"{article_id}_anti_echo_analysis.csv\"\n",
        "df.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"Detailed analysis saved to: {out_path}\")\n",
        "print(f\"Total matches found: {len(df)}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrJab83rIDvk",
        "outputId": "a7224928-48bf-4feb-f98e-8c72a4e9fa79"
      },
      "id": "ZrJab83rIDvk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running anti-echo retrieval for fox_test.pdf (bias=0.8, tone=0.0)\n",
            "\n",
            "Uploaded article topic summary: Trump imposes sanctions on Russian oil companies Rosneft and Lukoil and cancels summit with Putin amid Ukraine conflict.\n",
            "Composite vector shape: (1536,)\n",
            "  Base topic: (768,)\n",
            "  Summary: (768,)\n",
            "\n",
            "Retrieved from Chroma:\n",
            "  - 2168 topic vectors\n",
            "  - 432 stance vectors\n",
            "  - 432 unique articles\n",
            "\n",
            "Scanning corpus for matches...\n",
            "Note: Database contains mix of old (768-dim) and new (1536-dim) embeddings\n",
            "Found 68 topic vector matches from 18 unique articles\n",
            "After deduplication: 18 unique articles\n",
            "\n",
            "Final result: 18 unique articles matched\n",
            "\n",
            "================================================================================\n",
            "                          IDEOLOGICAL SPREAD OVERVIEW                           \n",
            "================================================================================\n",
            "\n",
            "Left / progressive outlets : none\n",
            "Right / conservative outlets: dailycaller, foxnews, reason\n",
            "\n",
            "Search parameters:\n",
            "  Your article summary: Trump imposes sanctions on Russian oil companies Rosneft and Lukoil and cancels summit with Putin amid Ukraine conflict.\n",
            "  Canonical topics: Politics / Global / Geopolitics & Conflict, Technology / Social Media & Platforms, Economy / Trade / Globalization...\n",
            "\n",
            "Top matching article: dailycaller (Conservative / Right)\n",
            "  Title: Trump Nominates New Person To Lead Commodity Futures Trading...\n",
            "  Canonical overlap: 0.62   Summary sim: 0.82   Stance sim: 0.50   Bias diff: 0.10\n",
            "  Anti-echo score: 1.375\n",
            "\n",
            "\n",
            "================================================================================\n",
            "                  SAME SPECIFIC TOPIC ‚Äî DIFFERENT SOURCE BIAS                   \n",
            "================================================================================\n",
            "\n",
            "‚Ä¢ Trump was planning to send troops to San Francisco. Now he‚Äôs not. Here‚Äôs why | Joe Eskenazi | The Guardian\n",
            "  Source: guardian  (Progressive / Left)\n",
            "  Canonical overlap: 0.44   Summary sim: 0.84   Stance sim: 0.59   Bias diff: 1.50\n",
            "  Anti-echo score: 0.109\n",
            "  Link: https://www.theguardian.com/commentisfree/2025/oct/25/san-francisco-tr‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "‚Ä¢ Untitled\n",
            "  Source: msnbc  (Progressive / Left)\n",
            "  Canonical overlap: 0.62   Summary sim: 0.83   Stance sim: 0.64   Bias diff: 1.50\n",
            "  Anti-echo score: 0.130\n",
            "  Link: https://www.msnbc.com/top-stories/latest/ronald-reagan-foundation-trum‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "‚Ä¢ Untitled\n",
            "  Source: msnbc  (Progressive / Left)\n",
            "  Canonical overlap: 0.30   Summary sim: 0.82   Stance sim: 0.62   Bias diff: 1.50\n",
            "  Anti-echo score: -0.036\n",
            "  Link: https://www.msnbc.com/rachel-maddow-show/maddowblog/trump-legality-dea‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "================================================================================\n",
            "                     SAME SPECIFIC TOPIC ‚Äî OPPOSITE STANCE                      \n",
            "================================================================================\n",
            "\n",
            "‚Ä¢ play\n",
            "  Source: aljazeera  (Center-Left)\n",
            "  Canonical overlap: 0.62   Summary sim: 0.83   Stance sim: 0.48   Bias diff: 1.00\n",
            "  Anti-echo score: 0.688\n",
            "  Link: https://www.aljazeera.com/news/2025/10/25/canadians-pull-reagan-advert‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "‚Ä¢ Trump Nominates New Person To Lead Commodity Futures Trading Commission | The Daily Caller\n",
            "  Source: dailycaller  (Conservative / Right)\n",
            "  Canonical overlap: 0.62   Summary sim: 0.82   Stance sim: 0.50   Bias diff: 0.10\n",
            "  Anti-echo score: 1.375\n",
            "  Link: https://dailycaller.com/2025/10/25/trump-nominates-mike-selig-chair-co‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "‚Ä¢ Untitled\n",
            "  Source: msnbc  (Progressive / Left)\n",
            "  Canonical overlap: 0.30   Summary sim: 0.81   Stance sim: 0.51   Bias diff: 1.50\n",
            "  Anti-echo score: 0.053\n",
            "  Link: https://www.msnbc.com/rachel-maddow-show/maddowblog/ag-pam-bondi-annou‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "================================================================================\n",
            "                TOP ANTI-ECHO CANDIDATES (BEST OVERALL MATCHES)                 \n",
            "================================================================================\n",
            "\n",
            "‚Ä¢ Trump Nominates New Person To Lead Commodity Futures Trading Commission | The Daily Caller\n",
            "  Source: dailycaller  (Conservative / Right)\n",
            "  Canonical overlap: 0.62   Summary sim: 0.82   Stance sim: 0.50   Bias diff: 0.10\n",
            "  Anti-echo score: 1.375\n",
            "  Link: https://dailycaller.com/2025/10/25/trump-nominates-mike-selig-chair-co‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "‚Ä¢ Trump meets world leaders, cancels Putin meeting before Asia trip | Fox News\n",
            "  Source: foxnews  (Conservative / Right)\n",
            "  Canonical overlap: 0.62   Summary sim: 0.84   Stance sim: 0.62   Bias diff: 0.00\n",
            "  Anti-echo score: 1.373\n",
            "  Link: https://www.foxnews.com/politics/trump-tears-down-east-wing-300m-ballr‚Ä¶\n",
            "--------------------------------------------------------------------------------\n",
            "‚Ä¢ Trump says he is ending tariff negotiations with Canada over Reagan speech video\n",
            "  Source: reason  (Center-Right)\n",
            "  Canonical overlap: 0.62   Summary sim: 0.82   Stance sim: 0.58   Bias diff: 0.30\n",
            "  Anti-echo score: 1.141\n",
            "  Link: https://reason.com/2025/10/24/canadas-antagonism/\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "Detailed analysis saved to: /content/unstructured-project/tmp/www.theguardian.com-at-least-four-killed-in-russian-strikes-overnight-on-ukraine-289dbfe2ed23_anti_echo_analysis.csv\n",
            "Total matches found: 18\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}